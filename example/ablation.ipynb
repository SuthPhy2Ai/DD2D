{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import * # to override the math functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from utils import set_seed, sample_from_model\n",
    "from trainer import Trainer, TrainerConfig\n",
    "from models_new import DD2D, TransformerConfig\n",
    "from utils import processDataFiles, CharDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists!\n"
     ]
    }
   ],
   "source": [
    "# set the random seed\n",
    "set_seed(42)\n",
    "# config\n",
    "device='cpu'\n",
    "scratch=True # if you want to ignore the cache and start for scratch\n",
    "numEpochs = 2000 # number of epochs to train the GPT+PT model\n",
    "embeddingSize = 384 # the hidden dimension of the representation of both GPT and PT\n",
    "numPoints = [20,350] # number of points that we are going to receive to make a prediction about f given x and y, if you don't know then use the maximum\n",
    "numVars = 2 # the dimenstion of input points x, if you don't know then use the maximum\n",
    "numYs = 1 # the dimension of output points y = f(x), if you don't know then use the maximum\n",
    "blockSize = 180 # spatial extent of the model for its context\n",
    "batchSize = 256 # batch size of training data\n",
    "target = 'Skeleton' #'Skeleton' #'EQ'\n",
    "dataDir = '../datasets/'\n",
    "\n",
    "data_set = 0\n",
    "if data_set == 0:\n",
    "    dataInfo = 'XYE_{}Var_{}-{}Points_{}EmbeddingSize'.format(numVars, numPoints[0], numPoints[1], embeddingSize)\n",
    "else:\n",
    "    dataInfo = 'XYE_{}Var_{}-{}Points_{}EmbeddingSize_dataset{}'.format(numVars, numPoints[0], numPoints[1], embeddingSize, data_set)\n",
    "\n",
    "titleTemplate = \"{} equations of {} variables - Benchmark\"\n",
    "addr = '../SavedModels/' # where to save model\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "method = 'EMB_SUM' # EMB_CAT/EMB_SUM/OUT_SUM/OUT_CAT/EMB_CON -> whether to concat the embedding or use summation. \n",
    "variableEmbedding = 'NOT_VAR' # NOT_VAR/LEA_EMB/STR_VAR\n",
    "# NOT_VAR: Do nothing, will not pass any information from the number of variables in the equation to the GPT\n",
    "# LEA_EMB: Learnable embedding for the variables, added to the pointNET embedding\n",
    "# STR_VAR: Add the number of variables to the first token\n",
    "addVars = True if variableEmbedding == 'STR_VAR' else False\n",
    "maxNumFiles = 100 # maximum number of file to load in memory for training the neural network\n",
    "bestLoss = None # if there is any model to load as pre-trained one\n",
    "fName = '{}_DD2D_{}_{}_{}_MINIMIZE.txt'.format(dataInfo, \n",
    "                                             'layer_heads_{}_{}'.format(n_layer, n_head), \n",
    "                                             'Padding',\n",
    "                                             variableEmbedding)\n",
    "ckptPath = '{}/{}.pt'.format(addr,fName.split('.txt')[0])\n",
    "try: \n",
    "    os.mkdir(addr)\n",
    "except:\n",
    "    print('Folder already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 6361 examples\n",
      "data has 795 examples\n",
      "data has 795 examples\n"
     ]
    }
   ],
   "source": [
    "# load the train dataset\n",
    "train_file = 'train_dataset_{}.pb'.format(fName)\n",
    "if os.path.isfile(train_file) and not scratch:\n",
    "    # just load the train set\n",
    "    with open(train_file, 'rb') as f:\n",
    "        train_dataset,trainText,chars = pickle.load(f)\n",
    "else:\n",
    "    path = f'../dataset/{data_set}.json'\n",
    "    files = glob.glob(path)[:maxNumFiles]\n",
    "    text = processDataFiles(files)\n",
    "\n",
    "    # chars = sorted(list(set(text)) + ['_', 'T', '<', '>', ':'])\n",
    "    text = text.split('\\n') # convert the raw text to a set of examples\n",
    "    trainText = text[:-1] if len(text[-1]) == 0 else text\n",
    "\n",
    "    random.shuffle(trainText) # shuffle the dataset, it's important specailly for the combined number of variables experiment\n",
    "    length = []\n",
    "    length_seq = []\n",
    "    values = []\n",
    "    for text in trainText:\n",
    "        text = json.loads(text)\n",
    "        text_x = text['X'][2]\n",
    "        len_i = len(text_x)\n",
    "        text_seq = text['seq']\n",
    "        text_seq = [x for x in text_seq if x != '']\n",
    "        len_j = len(text_seq)\n",
    "        length.append(len_i)\n",
    "        length_seq.append(len_j)\n",
    "        if len_i != 0:\n",
    "            value = np.max(text)\n",
    "            values.append(value)\n",
    "    numPoints = np.max(length) + 3\n",
    "    \n",
    "    val_size = int(len(trainText) * 0.1)\n",
    "    val_set = trainText[:val_size]\n",
    "    test_set = trainText[val_size : 2 * val_size]\n",
    "    # test_set = trainText[:500]\n",
    "    train_set = trainText[2 * val_size:]\n",
    "    train_dataset = CharDataset(train_set, blockSize, numVars=numVars,\n",
    "                    numYs=numYs, numPoints=numPoints)\n",
    "    val_dataset = CharDataset(val_set, blockSize, numVars=numVars,\n",
    "                    numYs=numYs, numPoints=numPoints)\n",
    "    test_dataset = CharDataset(test_set, blockSize, numVars=numVars,\n",
    "                    numYs=numYs, numPoints=numPoints)\n",
    "    # with open(train_file, 'wb') as f:\n",
    "    #     pickle.dump([train_dataset,trainText,chars], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq:tensor([ 2.0000e+02,  7.0123e+00,  9.0141e+00,  2.7145e+01,  9.0002e+01,\n",
      "         8.9998e+01,  8.9981e+01,  1.7424e+00,  6.5613e-02,  1.0028e+01,\n",
      "         1.7000e+01,  5.2617e+00,  7.8076e-02,  1.0035e+01,  1.7000e+01,\n",
      "         7.0090e+00,  6.6258e+00,  1.1571e+01,  1.7000e+01,  2.3537e-02,\n",
      "         2.3768e+00,  1.1698e+01,  1.7000e+01,  1.6594e+00,  8.9456e+00,\n",
      "         1.3408e+01,  1.7000e+01,  5.4033e+00,  8.9773e+00,  1.3423e+01,\n",
      "         1.7000e+01,  1.8799e+00,  4.4453e+00,  1.3718e+01,  1.7000e+01,\n",
      "         5.1567e+00,  4.4214e+00,  1.3738e+01,  1.7000e+01,  3.5342e+00,\n",
      "         6.8722e+00,  1.5433e+01,  1.7000e+01,  3.4823e+00,  2.1213e+00,\n",
      "         1.5594e+01,  1.7000e+01,  1.7453e+00,  4.6124e+00,  1.7095e+01,\n",
      "         1.7000e+01,  5.2454e+00,  4.5718e+00,  1.7114e+01,  1.7000e+01,\n",
      "         3.4994e+00,  2.1327e+00,  1.2113e+01,  1.9000e+01,  7.0197e+00,\n",
      "         6.6372e+00,  1.5022e+01,  1.9000e+01,  1.9593e-02,  8.9853e+00,\n",
      "         1.1615e+01,  4.1000e+01,  3.5062e+00,  4.4764e+00,  1.5525e+01,\n",
      "         4.1000e+01,  2.1000e+02, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00])\n",
      "points:tensor([[ 66.0000,  66.0000,  66.0000,  66.0000,  66.0000,  66.0000,  66.0000,\n",
      "          66.0000,  66.0000,  66.0000,  66.0000,  66.0000,  66.0000,  72.0000,\n",
      "          72.0000,  72.0000,  72.0000,  72.0000,  72.0000,  72.0000,  72.0000,\n",
      "          72.0000,  72.0000,  72.0000,  72.0000,  72.0000,  78.0000,  78.0000,\n",
      "          78.0000,  78.0000,  78.0000,  78.0000,  78.0000,  78.0000,  78.0000,\n",
      "          78.0000,  78.0000,  78.0000,  83.0000,  83.0000,  83.0000,  83.0000,\n",
      "          83.0000,  83.0000,  83.0000,  83.0000,  83.0000,  83.0000,  83.0000,\n",
      "          83.0000,  83.0000,  89.0000,  89.0000,  89.0000,  89.0000,  89.0000,\n",
      "          89.0000,  89.0000,  89.0000,  89.0000,  89.0000,  89.0000,  89.0000,\n",
      "          89.0000,  95.0000,  95.0000,  95.0000,  95.0000,  95.0000,  95.0000,\n",
      "          95.0000,  95.0000,  95.0000,  95.0000,  95.0000,  95.0000,  95.0000,\n",
      "         100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 101.0000,\n",
      "         101.0000, 101.0000, 101.0000, 101.0000, 101.0000, 101.0000, 106.0000,\n",
      "         106.0000, 106.0000, 106.0000, 106.0000, 106.0000, 106.0000, 106.0000,\n",
      "         106.0000, 106.0000, 106.0000, 106.0000, 106.0000, 112.0000, 112.0000,\n",
      "         112.0000, 112.0000, 112.0000, 112.0000, 112.0000, 112.0000, 112.0000,\n",
      "         112.0000, 112.0000, 112.0000, 112.0000, 118.0000, 118.0000, 118.0000,\n",
      "         118.0000, 118.0000, 118.0000, 118.0000, 118.0000, 118.0000, 118.0000,\n",
      "         118.0000, 118.0000, 118.0000, 123.0000, 123.0000, 123.0000, 123.0000,\n",
      "         123.0000, 123.0000, 123.0000, 123.0000, 123.0000, 123.0000, 123.0000,\n",
      "         123.0000, 123.0000, 129.0000, 129.0000, 129.0000, 129.0000, 129.0000,\n",
      "         129.0000, 129.0000, 129.0000, 129.0000, 129.0000, 129.0000, 129.0000,\n",
      "         129.0000, 135.0000, 135.0000, 135.0000, 135.0000, 135.0000, 135.0000,\n",
      "         135.0000, 135.0000, 135.0000, 135.0000, 135.0000, 135.0000, 135.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [ 74.0000,  78.0000,  83.0000,  87.0000,  92.0000,  96.0000, 101.0000,\n",
      "         105.0000, 109.0000, 114.0000, 118.0000, 123.0000, 127.0000,  74.0000,\n",
      "          78.0000,  83.0000,  87.0000,  92.0000,  96.0000, 101.0000, 105.0000,\n",
      "         109.0000, 114.0000, 118.0000, 123.0000, 127.0000,  74.0000,  83.0000,\n",
      "          87.0000,  92.0000,  96.0000, 101.0000, 105.0000, 109.0000, 114.0000,\n",
      "         118.0000, 123.0000, 127.0000,  74.0000,  78.0000,  83.0000,  87.0000,\n",
      "          92.0000,  96.0000, 101.0000, 105.0000, 109.0000, 114.0000, 118.0000,\n",
      "         123.0000, 127.0000,  74.0000,  78.0000,  83.0000,  87.0000,  92.0000,\n",
      "          96.0000, 101.0000, 105.0000, 109.0000, 114.0000, 118.0000, 123.0000,\n",
      "         127.0000,  74.0000,  78.0000,  83.0000,  87.0000,  92.0000,  96.0000,\n",
      "         101.0000, 105.0000, 109.0000, 114.0000, 118.0000, 123.0000, 127.0000,\n",
      "          74.0000,  78.0000,  83.0000,  87.0000,  92.0000,  96.0000, 101.0000,\n",
      "         105.0000, 109.0000, 114.0000, 118.0000, 123.0000, 127.0000,  74.0000,\n",
      "          78.0000,  83.0000,  87.0000,  92.0000,  96.0000, 100.0000, 105.0000,\n",
      "         109.0000, 114.0000, 118.0000, 123.0000, 127.0000,  74.0000,  78.0000,\n",
      "          83.0000,  87.0000,  92.0000,  96.0000, 100.0000, 105.0000, 109.0000,\n",
      "         114.0000, 118.0000, 123.0000, 127.0000,  74.0000,  78.0000,  83.0000,\n",
      "          87.0000,  92.0000,  96.0000, 100.0000, 105.0000, 109.0000, 114.0000,\n",
      "         118.0000, 123.0000, 127.0000,  74.0000,  78.0000,  83.0000,  87.0000,\n",
      "          92.0000,  96.0000, 100.0000, 105.0000, 109.0000, 114.0000, 118.0000,\n",
      "         123.0000, 127.0000,  74.0000,  78.0000,  83.0000,  87.0000,  92.0000,\n",
      "          96.0000, 100.0000, 105.0000, 109.0000, 114.0000, 118.0000, 123.0000,\n",
      "         127.0000,  74.0000,  78.0000,  83.0000,  87.0000,  92.0000,  96.0000,\n",
      "         100.0000, 105.0000, 109.0000, 114.0000, 118.0000, 123.0000, 127.0000,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-45.1660, -46.6249, -44.6078, -45.3043, -44.1911, -44.4339, -44.7092,\n",
      "         -44.9959, -44.7805, -44.8824, -45.9941, -45.3571, -45.1797, -44.6493,\n",
      "         -44.4695, -44.1093, -44.9478, -45.2386, -44.8617, -44.8332, -46.2126,\n",
      "         -45.4736, -45.8202, -45.1797, -44.4434, -46.1668, -44.9959, -45.4203,\n",
      "         -44.7502, -44.3613, -44.0873, -43.6292, -44.2634, -45.1763, -45.2505,\n",
      "         -45.0682, -44.2229, -44.8920, -44.3948, -44.8920, -46.1668, -45.1270,\n",
      "         -44.7805, -44.5767, -43.6386, -43.7059, -44.4788, -45.3009, -45.0256,\n",
      "         -45.5194, -45.8729, -45.5549, -46.9715, -44.9959, -44.2209, -45.6890,\n",
      "         -44.5947, -44.0716, -44.0383, -44.3223, -45.0545, -45.2046, -44.8526,\n",
      "         -44.4065, -44.7805, -44.5947, -44.6727, -44.7049, -45.4736, -45.4736,\n",
      "         -45.1270, -45.1494, -45.1660, -46.2706, -44.6078, -44.4695, -44.8617,\n",
      "         -44.3027, -44.8617, -44.8843, -45.1494, -44.4695, -44.5455, -44.1911,\n",
      "         -43.9754, -45.2046, -45.8202, -45.0997, -45.1797, -44.7502, -44.8843,\n",
      "         -45.4736, -44.9959, -45.6795, -45.1147, -44.5767, -44.6078, -44.5947,\n",
      "         -46.2783, -44.7852, -45.5263, -45.1660, -44.6841, -46.1668, -46.3822,\n",
      "         -46.1668, -45.2309, -44.7502, -44.8617, -44.6187, -43.8523, -44.0796,\n",
      "         -45.3571, -45.5549, -45.8202, -44.9222, -44.4927, -44.8045, -45.0880,\n",
      "         -44.8877, -44.3613, -44.4866, -46.2783, -46.0552, -45.8729, -45.2083,\n",
      "         -45.1147, -46.2783, -44.9959, -44.4216, -45.8729, -44.6985, -45.4433,\n",
      "         -45.1797, -44.0198, -43.7869, -44.1331, -44.5947, -44.4277, -46.1668,\n",
      "         -44.4831, -45.3621, -44.8617, -44.6689, -45.1660, -44.4065, -44.5573,\n",
      "         -44.4866, -44.0340, -43.7926, -44.4866, -46.0552, -44.9764, -44.7216,\n",
      "         -45.2386, -44.1431, -44.8332, -45.4674, -46.2126, -45.4736, -44.3299,\n",
      "         -44.4458, -44.6689, -44.2229, -44.3318, -45.6890, -45.5263, -44.8263,\n",
      "           3.0000,   0.0000,   0.0000,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(train_dataset.__len__())\n",
    "points, seq = train_dataset.__getitem__(idx)\n",
    "print('seq:{}'.format(seq))\n",
    "print('points:{}'.format(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/30/2024 00:44:26 - INFO - models_new -   number of parameters: 1.538074e+07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following model ../SavedModels//XYE_2Var_20-350Points_384EmbeddingSize_DD2D_layer_heads_4_4_Padding_NOT_VAR_MINIMIZE.pt has been loaded!\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "mconf = TransformerConfig(train_dataset.block_size,\n",
    "                  n_layer=n_layer, n_head=n_head, n_embd=embeddingSize)\n",
    "model = DD2D(mconf)\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=numEpochs, batch_size=batchSize, \n",
    "                      learning_rate=4e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, \n",
    "                      final_tokens=2*len(train_dataset)*blockSize,\n",
    "                      num_workers=0, ckpt_path=ckptPath)\n",
    "trainer = Trainer(model, train_dataset, val_dataset, tconf, bestLoss, device=device)\n",
    "print('The following model {} has been loaded!'.format(ckptPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(ckptPath)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.eval().to(trainer.device)\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    batch_size=len(test_dataset),\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy: 59.874213836477985\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(loader):\n",
    "    topN = 1\n",
    "    acc = sample_from_model(model, y, x, topN=topN)\n",
    "    print('Top {} accuracy: {}'.format(topN, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 accuracy: 89.18238993710692\n",
      "Top 5 accuracy: 96.72955974842768\n",
      "Top 10 accuracy: 99.74842767295597\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(loader):\n",
    "    topN = 3\n",
    "    acc = sample_from_model(model, y, x, topN=topN)\n",
    "    print('Top {} accuracy: {}'.format(topN, acc))\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    topN = 5\n",
    "    acc = sample_from_model(model, y, x, topN=topN)\n",
    "    print('Top {} accuracy: {}'.format(topN, acc))\n",
    "\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    topN = 10\n",
    "    acc = sample_from_model(model, y, x, topN=topN)\n",
    "    print('Top {} accuracy: {}'.format(topN, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
